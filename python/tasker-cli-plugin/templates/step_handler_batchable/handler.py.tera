"""{{ name | pascal_case }} batchable step handlers for parallel processing.

Workflow pattern:
    1. Analyzer step: returns BatchConfig to divide work into batches
    2. Worker steps: process individual batches in parallel
    3. Convergence step: aggregates results from all workers
"""

from tasker_core.step_handler.functional import (
    BatchConfig,
    batch_analyzer,
    batch_worker,
    depends_on,
    inputs,
    step_handler,
)


@batch_analyzer("{{ name | snake_case }}_analyzer", worker_template="{{ name | snake_case }}_batch")
@inputs("total_items")
def {{ name | snake_case }}_analyzer(total_items, context):
    """{{ name | pascal_case }} batch analyzer.

    Determines total work items and batch size. The DSL automatically
    generates cursor configs and creates the batch processing outcome.

    Args:
        total_items: value from task input "total_items"
        context: step execution context

    Register this handler in your task definition YAML:
        - name: {{ name | snake_case }}_analyze
          type: batchable
          handler:
              callable: {{ module_name }}.{{ name | snake_case }}_handler.{{ name | snake_case }}_analyzer
    """
    # TODO: Determine total items from your data source
    count = int(total_items or 1000)
    batch_size = 200

    return BatchConfig(total_items=count, batch_size=batch_size)


@batch_worker("{{ name | snake_case }}_worker")
def {{ name | snake_case }}_worker(batch_context, context):
    """{{ name | pascal_case }} batch worker.

    Processes a single batch of items within the cursor range assigned
    by the analyzer step. The batch_context is auto-injected by the DSL.

    Register this handler in your task definition YAML:
        - name: {{ name | snake_case }}_batch
          type: batch_worker
          dependencies: [{{ name | snake_case }}_analyze]
          handler:
              callable: {{ module_name }}.{{ name | snake_case }}_handler.{{ name | snake_case }}_worker
    """
    # TODO: Process items in the batch range
    items_processed = batch_context.cursor_config.end_cursor - batch_context.cursor_config.start_cursor

    return {
        "items_processed": items_processed,
        "batch_id": batch_context.cursor_config.batch_id,
    }


@step_handler("{{ name | snake_case }}_convergence")
def {{ name | snake_case }}_convergence(context):
    """{{ name | pascal_case }} convergence step â€” aggregates results from all batch workers.

    Runs automatically after all batch workers complete. Iterates
    dependency results by worker name prefix to collect and aggregate.

    Register this handler in your task definition YAML:
        - name: {{ name | snake_case }}_converge
          type: deferred_convergence
          dependencies: [{{ name | snake_case }}_batch]
          handler:
              callable: {{ module_name }}.{{ name | snake_case }}_handler.{{ name | snake_case }}_convergence
    """
    total_processed = 0
    worker_count = 0

    for dep_name in context.dependency_results:
        if dep_name.startswith("{{ name | snake_case }}_batch_"):
            result = context.get_dependency_result(dep_name)
            if result is not None and isinstance(result, dict):
                total_processed += result.get("items_processed", 0)
                worker_count += 1

    # TODO: Add your domain-specific aggregation logic here
    return {
        "total_processed": total_processed,
        "worker_count": worker_count,
    }
